{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e67ac1-a935-4352-9940-34979abed5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-importing necessary packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import statsmodels.api as sm\n",
    "from pycaret.regression import *\n",
    "import xgboost as xgb\n",
    "import catboost as ctb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "#!pip install imageio\n",
    "#import imageio\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "print('Importing libraries: Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a60b8a1-2a6d-4437-846b-00cbbf2e6294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-Inputs operation\n",
    "\n",
    "# checking inputs\n",
    "print(\"Folder's files : \",os.listdir('inputs'), '\\n', '_________','\\n')\n",
    "\n",
    "# Reading input CSV files and assigning a name to each one of them \n",
    "dataset = pd.read_csv(\"inputs/train.csv\", names=['Store','Dept','Date','weeklySales','isHoliday'],sep=',', header=0)\n",
    "features = pd.read_csv(\"inputs/features.csv\",sep=',', header=0,names=['Store','Date','Temperature','Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5','CPI','Unemployment','IsHoliday']).drop(columns=['IsHoliday'])\n",
    "stores = pd.read_csv(\"inputs/stores.csv\", names=['Store','Type','Size'],sep=',', header=0)\n",
    "\n",
    "# Making needed directories\n",
    "os.makedirs('temp_test', exist_ok=True)\n",
    "os.makedirs('input_analysis', exist_ok=True)\n",
    "os.makedirs('pred_output', exist_ok=True)\n",
    "os.makedirs('output_analysis', exist_ok=True)\n",
    "\n",
    "# Flating data(merging different data bases into one table)\n",
    "dataset = dataset.merge(stores, how='left').merge(features, how='left')  \n",
    "print('Original dataset sample:', '\\n', dataset.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b275fd-9cdd-4b84-b131-2843b63b8a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decreasing unnecessary memory usage \n",
    "dataset['Store'] = dataset['Store'].astype('int16')\n",
    "dataset['Dept'] = dataset['Dept'].astype('int16')\n",
    "dataset['weeklySales'] = dataset['weeklySales'].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da5323c-bc09-467d-a311-465e222e2116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-Data extraction\n",
    "\n",
    "# Deriving a sub-dataset number 1 from flat dataset\n",
    "dataset_sub1 = dataset[['Date','Store','Dept','weeklySales']]\n",
    "dataset_sub1 = dataset_sub1.sort_index(axis=0)\n",
    "dataset_sub1.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fb329d-bafb-4c49-9438-4b3f3b7a95c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting list of unique departments' values\n",
    "dept_list = dataset_sub1['Dept'].unique()\n",
    "dept_list.sort()\n",
    "dept_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3bc2b1-f538-43b0-b7ba-4924a2a5a043",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting list of unique stores' values\n",
    "store_list = dataset_sub1['Store'].unique()\n",
    "store_list.sort()\n",
    "store_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e21c042-f4f9-47e9-82f0-d86bed78b7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting list of unique dates\n",
    "date_list = dataset_sub1['Date'].unique()\n",
    "date_list.sort()\n",
    "date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b11e606-c710-4bad-93ea-eacac174cbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function removes departmets in different stores which are not complete or has below 0 sales value\n",
    "def outlier_identifier(df, border_value, store_list, dept_list):\n",
    "    data_map = pd.DataFrame(columns=['store', 'dept', 'number_of_entries', 'target_false_count', 'outlier_flag'])\n",
    "    for i in store_list:\n",
    "        for j in dept_list:\n",
    "            number_of_entries = df[(df.Store == i) & (df.Dept == j)].Date.count()\n",
    "            number_of_entries = number_of_entries.astype('int16')\n",
    "            target_false_count = df[(df.weeklySales <= border_value) & (df.Store == \n",
    "                                                                        i) & (df.Dept == j)].weeklySales.count()\n",
    "            target_false_count = target_false_count.astype('int16')\n",
    "            if (number_of_entries == 143) & (target_false_count == 0):\n",
    "                outlier_flag = 0\n",
    "            else:\n",
    "                outlier_flag = 1\n",
    "            new_row = {'store': i, 'dept': j, 'number_of_entries': number_of_entries, 'target_false_count': target_false_count, 'outlier_flag': outlier_flag}\n",
    "            data_map.loc[len(data_map)] = new_row\n",
    "    return data_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd9d641-df1a-49f6-b6aa-53065188c4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_map = outlier_identifier(dataset_sub1, 0, store_list, dept_list)\n",
    "data_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37531d2-73f8-49b4-aef6-4a110041bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('outlier percentage:', data_map[data_map.outlier_flag == 1].store.count() / 3645 , '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740de506-db0c-49d4-80b1-48f9bfecc3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a mapping dataframe to know which store and department mix should be droped\n",
    "removal_map = data_map[['store','dept','outlier_flag']]\n",
    "removal_map = removal_map[removal_map.outlier_flag == 1]\n",
    "removal_map.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686db6d9-3fe7-4bc5-ac25-45786f7abdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_remover(df, removal_map):\n",
    "    for i in removal_map.index:\n",
    "        a = removal_map.iloc[[i]].store\n",
    "        a.reset_index(drop=True, inplace=True)\n",
    "        b = removal_map.iloc[[i]].dept\n",
    "        b.reset_index(drop=True, inplace=True)\n",
    "        print('Store:', a[0], 'Department:', b[0],' Removed as outlier!','\\n')\n",
    "        index = df[(df.Store == a[0]) & (df.Dept == b[0])].index\n",
    "        for j in index:\n",
    "            df.drop(j , inplace=True)\n",
    "#            print('droped index:', j)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dc2b35-5150-400d-b677-be2a4e356699",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sub2 = outlier_remover(dataset_sub1, removal_map)\n",
    "dataset_sub2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e90324-4649-43ef-b7f7-f1146cc969d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sub2.to_csv('temp_test/dataset_sub2.csv') #outlier removing is time consuming, therefore we save it and skip cleaning next time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46197aac-b7de-4205-9c22-8bda076ddb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sub2 = pd.read_csv(\"temp_test/dataset_sub2.csv\", names=['Date','Store','Dept','weeklySales'],sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2495b9c0-952b-48e1-8744-a20e61ebb2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting list of unique departments' values\n",
    "dept_list = dataset_sub2['Dept'].unique()\n",
    "dept_list = dept_list.sort()\n",
    "dept_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e75f1a-d8ab-4bba-9d7b-89aa01dd1361",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdls = ['dt', 'rf', 'et', 'xgboost', 'catboost', 'lightgbm', 'gbr', 'huber', 'ada', 'par', 'omp', 'en', 'lasso', 'llar', 'br', 'ridge', 'lar', 'lr']\n",
    "# excluded: 'dummy', 'knn'\n",
    "#mdls = ['ada','dt']\n",
    "#dept_list = [1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5dcc44-2ffe-4234-8a78-ee348b2ada2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function slices input dataframe acording to values of a column (Test:OK)\n",
    "def slicer(df, dept):\n",
    "    x = df[df['Dept'] == dept].copy()\n",
    "    x = x.drop(['Dept'], axis=1)\n",
    "    return x \n",
    "\n",
    "\n",
    "#This function (Test:OK)\n",
    "def aggregator(df):\n",
    "    aggr = df.groupby('Date', as_index=True).sum()\n",
    "    aggr.index = pd.to_datetime(aggr.index)\n",
    "    return aggr\n",
    "\n",
    "#This function (Test:OK)\n",
    "def create_features(df):\n",
    "    df['Date'] = df.index\n",
    "    df['dayofweek'] = df['Date'].dt.dayofweek\n",
    "    df['quarter'] = df['Date'].dt.quarter\n",
    "    df['month'] = df['Date'].dt.month\n",
    "    df['year'] = df['Date'].dt.year\n",
    "    df['dayofyear'] = df['Date'].dt.dayofyear\n",
    "    df['dayofmonth'] = df['Date'].dt.day\n",
    "    df['weekofyear'] = df['Date'].dt.weekofyear\n",
    "    df['dayofweek'] = df['dayofweek'].astype('int16')\n",
    "    df['quarter'] = df['quarter'].astype('int16')\n",
    "    df['month'] = df['month'].astype('int16')\n",
    "    df['year'] = df['year'].astype('int16')\n",
    "    df['dayofyear'] = df['dayofyear'].astype('int16')\n",
    "    df['dayofmonth'] = df['dayofmonth'].astype('int16')\n",
    "    df['weekofyear'] = df['weekofyear'].astype('int16')\n",
    "    X = df[['dayofweek','quarter','month','year','dayofyear','dayofmonth','weekofyear', 'weeklySales']]\n",
    "    X.index = df.index\n",
    "    return X\n",
    "\n",
    "#This function (Test:OK)\n",
    "def split_data(df, split_date):\n",
    "    return df[df.index < split_date].copy(), \\\n",
    "            df[df.index >= split_date].copy()\n",
    "\n",
    "#This function plots test and train values of target in time (Test:OK)\n",
    "def plt_test_train(df_train, df_test):\n",
    "    plt.figure(figsize = (20,10))\n",
    "    plt.xlabel('date')\n",
    "    plt.ylabel('weekly sales')\n",
    "    plt.plot(df_train.index, df_train['weeklySales'],label = 'train')\n",
    "    plt.plot(df_test.index, df_test['weeklySales'], label ='test')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    msg = 'PLT Done ! \\n'\n",
    "    return msg\n",
    "\n",
    "#This function creates, tunes, plots, finalizes, predicts, and evaluates all models in mdls list for a set of data\n",
    "def create_models(mdls, i, test, result_log, result_log_aggr):\n",
    "    j = str(i)\n",
    "    for mdl in mdls:\n",
    "        mdll = create_model('{}'.format(mdl))\n",
    "        print('\\n \\n >>mdll = create_model(mdl) for Dept:{} Model:{} IS  DONE! \\n \\n'.format(j,mdl))\n",
    "        tuned_mdl = tune_model(mdll, fold = None, n_iter = 20)\n",
    "        print('\\n \\n >>tuned_mdl = tune_model(mdll) for Dept:{} Model:{} IS  DONE! \\n \\n'.format(j,mdl))\n",
    "        plot_model(mdll)\n",
    "        print('\\n \\n >>plot_model(mdll) for Dept:{} Model:{} IS  DONE! \\n \\n'.format(j,mdl))\n",
    "        plot_model(mdll, plot = 'error')\n",
    "        print('\\n \\n >>plot_model(mdll, plot = error) for Dept:{} Model:{} IS  DONE! \\n \\n'.format(j,mdl))\n",
    "        plot_model(tuned_mdl, plot = 'feature')\n",
    "        print('\\n \\n >>plot_model(tuned_mdl, plot = feature) for Dept:{} Model:{} IS  DONE! \\n \\n'.format(j,mdl))\n",
    "        predict_model(tuned_mdl);\n",
    "        print('\\n \\n >>predict_model(tuned_mdl) for Dept:{} Model:{} IS  DONE! \\n \\n'.format(j,mdl))\n",
    "        final_mdl = finalize_model(tuned_mdl)\n",
    "        print('\\n \\n >>final_mdl = finalize_model(tuned_mdl) for Dept:{} Model:{} IS  DONE! \\n \\n'.format(j,mdl))\n",
    "        print(final_mdl)\n",
    "        print('\\n \\n >>print(final_mdl) for Dept:{} Model:{} IS  DONE! \\n \\n'.format(j,mdl))\n",
    "        evaluate_model(final_mdl)\n",
    "        print('\\n \\n >>evaluate_model(final_mdl) for Dept:{} Model:{} IS  DONE! \\n \\n'.format(j,mdl))\n",
    "        predict_model(final_mdl)\n",
    "        print('\\n \\n >>predict_model(final_mdl) for Dept:{} Model:{} IS  DONE! \\n \\n'.format(j,mdl))\n",
    "        pred_mdl = predict_model(final_mdl, data=test)\n",
    "        pred_mdl.to_csv('pred_output/{}_{}.csv'.format(j,mdl))\n",
    "        print('\\n \\n >>pred_mdl = predict_model(final_mdl, data=test) for Dept:{} Model:{} IS  DONE! \\n \\n'.format(j,mdl))\n",
    "        result_log[mdl] = pred_mdl.prediction_label\n",
    "        result_log['{}_ape'.format(mdl)] = ((result_log.actual - result_log[mdl]) / result_log.actual).abs() \n",
    "        result_log['{}_pe'.format(mdl)] = ((result_log.actual - result_log[mdl]) / result_log.actual) \n",
    "        result_log['{}_pos_pe'.format(mdl)] = result_log[(result_log['{}_pe'.format(mdl)] >= 0)]['{}_pe'.format(mdl)]\n",
    "        result_log['{}_neg_pe'.format(mdl)] = result_log[(result_log['{}_pe'.format(mdl)] < 0)]['{}_pe'.format(mdl)]\n",
    "        pos_pe_sum = result_log['{}_pos_pe'.format(mdl)].sum()\n",
    "        max_pos_pe = result_log['{}_pos_pe'.format(mdl)].max()\n",
    "        neg_pe_sum = result_log['{}_neg_pe'.format(mdl)].sum()\n",
    "        max_neg_pe = result_log['{}_neg_pe'.format(mdl)].min()\n",
    "        mape = result_log['{}_ape'.format(mdl)].mean()\n",
    "        result_log_aggr.at[i, '{}_pos_pe_sum'.format(mdl)] = pos_pe_sum\n",
    "        result_log_aggr.at[i, '{}_max_pos_pe'.format(mdl)] = max_pos_pe\n",
    "        result_log_aggr.at[i, '{}_neg_pe_sum'.format(mdl)] = neg_pe_sum\n",
    "        result_log_aggr.at[i, '{}_max_neg_pe'.format(mdl)] = max_neg_pe\n",
    "        result_log_aggr.at[i, '{}_mape'.format(mdl)] = mape\n",
    "        \n",
    "        #dept_mape_list.append(result_log['{}_ape'.format(mdl)].mean())\n",
    "        #result_log_aggr = dept_mape_list.add(mape)\n",
    "        print('\\n \\n >> Prediction j_mdl.to_csv for Dept:{} Model:{}  IS  DONE! \\n \\n'.format(j,mdl))\n",
    "    return result_log, result_log_aggr\n",
    "    \n",
    "        \n",
    "#This function \n",
    "def mlsetup(train, test, j):\n",
    "    reg = setup(data = train,\n",
    "            test_data = test,\n",
    "            target = 'weeklySales',\n",
    "            #categorical_features = ['Dept','Store'],\n",
    "            numeric_features = ['dayofweek','quarter','month','year','dayofyear','dayofmonth','weekofyear'],\n",
    "            preprocess = False,\n",
    "            imputation_type = None, #We dont want to impute missing values because they are alreay imputed.\n",
    "            #numeric_imputation = 'mean',\n",
    "            polynomial_features = False, #it means we do not want to take existing features and rase them to a power to capture non-linear relationships between the feature and the target variable.\n",
    "            transformation = False,\n",
    "            normalize = False,\n",
    "            #normalize_method = 'zscore',\n",
    "            transform_target = False,\n",
    "            remove_multicollinearity = False,\n",
    "            #multicollinearity_threshold = 0.95,\n",
    "            remove_outliers = False,\n",
    "            #outliers_method = 'ee' #options are 'ee', 'lof', 'iforest',\n",
    "            #outliers_threshhold = 0.05,\n",
    "            feature_selection = False,\n",
    "            #feature_selection_method = 'sequential',\n",
    "            #feature_selection_estimator = 'lightgbm',\n",
    "            #n_features_to_select = 0.2,\n",
    "            #use_gpu = True,\n",
    "            #profile = True,\n",
    "            fold_strategy = 'timeseries', #other options are 'kfold', 'groupkfold', 'timeseries'\n",
    "            fold = 2,  \n",
    "            #fold_groups = 'dept',\n",
    "            data_split_shuffle = False,\n",
    "            fold_shuffle = False,\n",
    "           )\n",
    "    print('\\n \\n >>ML setup for Dept:{} IS  DONE! \\n \\n'.format(j))\n",
    "    best = compare_models(sort = 'MAPE', n_select = 1)\n",
    "    print('\\n \\n >>best = compare_models for Dept:{} IS  DONE! \\n \\n'.format(j))\n",
    "    evaluate_model(best)\n",
    "    print('\\n \\n >>evaluate_model(best) for Dept:{} IS  DONE! \\n \\n'.format(j))\n",
    "    return best\n",
    "\n",
    "#This function\n",
    "#def err_calc():\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#This function executes functions of machine learning for each pipe and changes the pipeline each time \n",
    "def process(df, num_dict, split_date, mdls):\n",
    "    result_log_aggr = pd.DataFrame(index=num_dict)\n",
    "    result_log_aggr['dept'] = num_dict\n",
    "    #result_log_aggr.index = num_dict.index\n",
    "    for i in num_dict:\n",
    "        j = str(i)\n",
    "        x = slicer(df, i)\n",
    "        y = aggregator(x)\n",
    "        z = create_features(y)\n",
    "        #z['dept'] = i\n",
    "        train, test = split_data(z, split_date)\n",
    "        print('>slicer, aggregator, create features, and split_data func for Dept:', i,'is Done! \\n')\n",
    "        plt_test_train(train, test)\n",
    "        print('\\n >plt_test_train func for Dept:', i,'is Done! \\n')\n",
    "        mlsetup(train, test, j)\n",
    "        print('\\n >mlsetup func for Dept:', i,'is Done! \\n')\n",
    "        result_log = pd.DataFrame()\n",
    "        result_log.index = test.index\n",
    "        result_log['actual'] = test['weeklySales']\n",
    "        result_log_dept, result_log_aggr = create_models(mdls, i, test, result_log, result_log_aggr)\n",
    "        result_log_dept.to_csv('output_analysis/result_log_dept{}.csv'.format(j))\n",
    "        print('\\n >create_models func for Dept:', i,'is Done! \\n')\n",
    "        \n",
    "    result_log_aggr.to_csv('output_analysis/result_log_aggr.csv')\n",
    "    process_end_msg = '>>>>>>>>>>> Prediction DONE! <<<<<<<<<<<<'\n",
    "    print(process_end_msg)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb6f833-b335-40c4-8034-499d305a838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "process(dataset_sub2, dept_list, '2011-10-19', mdls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
