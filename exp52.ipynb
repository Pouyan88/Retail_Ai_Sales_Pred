{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "833b398a-f7a7-4ad5-8f31-deacb95b5e91",
   "metadata": {},
   "source": [
    "# Sales pred simple ranking  \n",
    "## ***Experience*** **52**:  \n",
    "### -aggregated sales in different stores  \n",
    "### -single-general learner  \n",
    "### -without markdowns?  \n",
    "\n",
    "## Contributions:\n",
    "### - wide range of methods \n",
    "### - department based error(p_err and n_err) calculation\n",
    "### - waited error ranking (check EXPLAIN method in exact folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39e67ac1-a935-4352-9940-34979abed5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing libraries: Done\n"
     ]
    }
   ],
   "source": [
    "# 0-importing necessary packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import statsmodels.api as sm\n",
    "from pycaret.regression import *\n",
    "import xgboost as xgb\n",
    "import catboost as ctb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import shap\n",
    "print('Importing libraries: Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a60b8a1-2a6d-4437-846b-00cbbf2e6294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder's files :  ['features.csv', 'inputs.rar', 'inputs.zip', 'stores.csv', 'test.csv', 'train.csv'] \n",
      "\n",
      "──────────────────────────────────────────────────────────────────────────────────────────────────── \n",
      " Original dataset sample: \n",
      "         Store  Dept        Date  weeklySales  isHoliday Type    Size  \\\n",
      "0           1     1  2010-02-05     24924.50      False    A  151315   \n",
      "1           1     1  2010-02-12     46039.49       True    A  151315   \n",
      "2           1     1  2010-02-19     41595.55      False    A  151315   \n",
      "3           1     1  2010-02-26     19403.54      False    A  151315   \n",
      "4           1     1  2010-03-05     21827.90      False    A  151315   \n",
      "...       ...   ...         ...          ...        ...  ...     ...   \n",
      "421565     45    98  2012-09-28       508.37      False    B  118221   \n",
      "421566     45    98  2012-10-05       628.10      False    B  118221   \n",
      "421567     45    98  2012-10-12      1061.02      False    B  118221   \n",
      "421568     45    98  2012-10-19       760.01      False    B  118221   \n",
      "421569     45    98  2012-10-26      1076.80      False    B  118221   \n",
      "\n",
      "        Temperature  Fuel_Price  MarkDown1  MarkDown2  MarkDown3  MarkDown4  \\\n",
      "0             42.31       2.572        NaN        NaN        NaN        NaN   \n",
      "1             38.51       2.548        NaN        NaN        NaN        NaN   \n",
      "2             39.93       2.514        NaN        NaN        NaN        NaN   \n",
      "3             46.63       2.561        NaN        NaN        NaN        NaN   \n",
      "4             46.50       2.625        NaN        NaN        NaN        NaN   \n",
      "...             ...         ...        ...        ...        ...        ...   \n",
      "421565        64.88       3.997    4556.61      20.64       1.50    1601.01   \n",
      "421566        64.89       3.985    5046.74        NaN      18.82    2253.43   \n",
      "421567        54.47       4.000    1956.28        NaN       7.89     599.32   \n",
      "421568        56.47       3.969    2004.02        NaN       3.18     437.73   \n",
      "421569        58.85       3.882    4018.91      58.08     100.00     211.94   \n",
      "\n",
      "        MarkDown5         CPI  Unemployment  \n",
      "0             NaN  211.096358         8.106  \n",
      "1             NaN  211.242170         8.106  \n",
      "2             NaN  211.289143         8.106  \n",
      "3             NaN  211.319643         8.106  \n",
      "4             NaN  211.350143         8.106  \n",
      "...           ...         ...           ...  \n",
      "421565    3288.25  192.013558         8.684  \n",
      "421566    2340.01  192.170412         8.667  \n",
      "421567    3990.54  192.327265         8.667  \n",
      "421568    1537.49  192.330854         8.667  \n",
      "421569     858.33  192.308899         8.667  \n",
      "\n",
      "[421570 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "# 1-Inputs operation\n",
    "\n",
    "\n",
    "# 1-1 Checking inputs\n",
    "print(\"Folder's files : \",os.listdir('inputs'), '\\n')\n",
    "\n",
    "# 1-2 Reading input CSV files and assigning a name to each one of them \n",
    "dataset = pd.read_csv(\"inputs/train.csv\", names=['Store','Dept','Date','weeklySales','isHoliday'],sep=',', header=0)\n",
    "features = pd.read_csv(\"inputs/features.csv\",sep=',', header=0,names=['Store','Date','Temperature','Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5','CPI','Unemployment','IsHoliday']).drop(columns=['IsHoliday'])\n",
    "stores = pd.read_csv(\"inputs/stores.csv\", names=['Store','Type','Size'],sep=',', header=0)\n",
    "\n",
    "# 1-3 Creating needed directories\n",
    "os.makedirs('temp_test', exist_ok=True)\n",
    "os.makedirs('input_analysis', exist_ok=True)\n",
    "os.makedirs('pred_output', exist_ok=True)\n",
    "os.makedirs('pred_output/exp52', exist_ok=True)\n",
    "os.makedirs('output_analysis', exist_ok=True)\n",
    "os.makedirs('output_analysis/exp52', exist_ok=True)\n",
    "\n",
    "# 1-4 Flating data(merging different data bases into one table)\n",
    "dataset = dataset.merge(stores, how='left').merge(features, how='left')  \n",
    "\n",
    "# 1-5 Decreasing unnecessary memory usage \n",
    "dataset['Store'] = dataset['Store'].astype('int16')\n",
    "dataset['Dept'] = dataset['Dept'].astype('int16')\n",
    "dataset['weeklySales'] = dataset['weeklySales'].astype('float64')\n",
    "\n",
    "# 1-6 Printing flatted dataset\n",
    "print('─' * 100,'\\n Original dataset sample: \\n', dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4da5323c-bc09-467d-a311-465e222e2116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset_sub1: \n",
      "               Date  Dept  Store Type  MarkDown1  MarkDown2  MarkDown3  \\\n",
      "0       2010-02-05     1      1    A        NaN        NaN        NaN   \n",
      "1       2010-02-12     1      1    A        NaN        NaN        NaN   \n",
      "2       2010-02-19     1      1    A        NaN        NaN        NaN   \n",
      "3       2010-02-26     1      1    A        NaN        NaN        NaN   \n",
      "4       2010-03-05     1      1    A        NaN        NaN        NaN   \n",
      "...            ...   ...    ...  ...        ...        ...        ...   \n",
      "421565  2012-09-28    98     45    B    4556.61      20.64       1.50   \n",
      "421566  2012-10-05    98     45    B    5046.74        NaN      18.82   \n",
      "421567  2012-10-12    98     45    B    1956.28        NaN       7.89   \n",
      "421568  2012-10-19    98     45    B    2004.02        NaN       3.18   \n",
      "421569  2012-10-26    98     45    B    4018.91      58.08     100.00   \n",
      "\n",
      "        MarkDown4  MarkDown5  isHoliday  weeklySales  \n",
      "0             NaN        NaN      False     24924.50  \n",
      "1             NaN        NaN       True     46039.49  \n",
      "2             NaN        NaN      False     41595.55  \n",
      "3             NaN        NaN      False     19403.54  \n",
      "4             NaN        NaN      False     21827.90  \n",
      "...           ...        ...        ...          ...  \n",
      "421565    1601.01    3288.25      False       508.37  \n",
      "421566    2253.43    2340.01      False       628.10  \n",
      "421567     599.32    3990.54      False      1061.02  \n",
      "421568     437.73    1537.49      False       760.01  \n",
      "421569     211.94     858.33      False      1076.80  \n",
      "\n",
      "[421570 rows x 11 columns]\n",
      "──────────────────────────────────────────────────────────────────────────────────────────────────── \n",
      " List of Departments: \n",
      " [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 50 51 52 54 55 56 58 59 60 65 67 71 72 74 77 78 79 80 81 82 83 85 87 90\n",
      " 91 92 93 94 95 96 97 98 99] \n",
      "\n",
      "──────────────────────────────────────────────────────────────────────────────────────────────────── \n",
      " List of Stores: \n",
      " [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45] \n",
      "\n",
      "──────────────────────────────────────────────────────────────────────────────────────────────────── \n",
      " List of Dates: \n",
      " ['2010-02-05' '2010-02-12' '2010-02-19' '2010-02-26' '2010-03-05'\n",
      " '2010-03-12' '2010-03-19' '2010-03-26' '2010-04-02' '2010-04-09'\n",
      " '2010-04-16' '2010-04-23' '2010-04-30' '2010-05-07' '2010-05-14'\n",
      " '2010-05-21' '2010-05-28' '2010-06-04' '2010-06-11' '2010-06-18'\n",
      " '2010-06-25' '2010-07-02' '2010-07-09' '2010-07-16' '2010-07-23'\n",
      " '2010-07-30' '2010-08-06' '2010-08-13' '2010-08-20' '2010-08-27'\n",
      " '2010-09-03' '2010-09-10' '2010-09-17' '2010-09-24' '2010-10-01'\n",
      " '2010-10-08' '2010-10-15' '2010-10-22' '2010-10-29' '2010-11-05'\n",
      " '2010-11-12' '2010-11-19' '2010-11-26' '2010-12-03' '2010-12-10'\n",
      " '2010-12-17' '2010-12-24' '2010-12-31' '2011-01-07' '2011-01-14'\n",
      " '2011-01-21' '2011-01-28' '2011-02-04' '2011-02-11' '2011-02-18'\n",
      " '2011-02-25' '2011-03-04' '2011-03-11' '2011-03-18' '2011-03-25'\n",
      " '2011-04-01' '2011-04-08' '2011-04-15' '2011-04-22' '2011-04-29'\n",
      " '2011-05-06' '2011-05-13' '2011-05-20' '2011-05-27' '2011-06-03'\n",
      " '2011-06-10' '2011-06-17' '2011-06-24' '2011-07-01' '2011-07-08'\n",
      " '2011-07-15' '2011-07-22' '2011-07-29' '2011-08-05' '2011-08-12'\n",
      " '2011-08-19' '2011-08-26' '2011-09-02' '2011-09-09' '2011-09-16'\n",
      " '2011-09-23' '2011-09-30' '2011-10-07' '2011-10-14' '2011-10-21'\n",
      " '2011-10-28' '2011-11-04' '2011-11-11' '2011-11-18' '2011-11-25'\n",
      " '2011-12-02' '2011-12-09' '2011-12-16' '2011-12-23' '2011-12-30'\n",
      " '2012-01-06' '2012-01-13' '2012-01-20' '2012-01-27' '2012-02-03'\n",
      " '2012-02-10' '2012-02-17' '2012-02-24' '2012-03-02' '2012-03-09'\n",
      " '2012-03-16' '2012-03-23' '2012-03-30' '2012-04-06' '2012-04-13'\n",
      " '2012-04-20' '2012-04-27' '2012-05-04' '2012-05-11' '2012-05-18'\n",
      " '2012-05-25' '2012-06-01' '2012-06-08' '2012-06-15' '2012-06-22'\n",
      " '2012-06-29' '2012-07-06' '2012-07-13' '2012-07-20' '2012-07-27'\n",
      " '2012-08-03' '2012-08-10' '2012-08-17' '2012-08-24' '2012-08-31'\n",
      " '2012-09-07' '2012-09-14' '2012-09-21' '2012-09-28' '2012-10-05'\n",
      " '2012-10-12' '2012-10-19' '2012-10-26']\n"
     ]
    }
   ],
   "source": [
    "# 2-Data extraction\n",
    "\n",
    "# 2-1 Deriving a sub-dataset from main dataset \n",
    "dataset_sub1 = dataset[['Date','Dept','Store', 'Type','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5','isHoliday','weeklySales']]\n",
    "dataset_sub1 = dataset_sub1.sort_index(axis=0)\n",
    "\n",
    "# 2-2 getting list of unique departments' values\n",
    "dept_list = dataset_sub1['Dept'].unique()\n",
    "dept_list.sort()\n",
    "\n",
    "# 2-3 getting list of unique stores' values\n",
    "store_list = dataset_sub1['Store'].unique()\n",
    "store_list.sort()\n",
    "\n",
    "# 2-4 getting list of unique dates\n",
    "date_list = dataset_sub1['Date'].unique()\n",
    "date_list.sort()\n",
    "\n",
    "# 2-5 Check printing \n",
    "print('Dataset_sub1: \\n',dataset_sub1)\n",
    "print('─' * 100,'\\n List of Departments: \\n',dept_list,'\\n')\n",
    "print('─' * 100,'\\n List of Stores: \\n',store_list,'\\n')\n",
    "print('─' * 100,'\\n List of Dates: \\n',date_list)\n",
    "\n",
    "# Deriving a sub-dataset from main dataset which considers 9 more important features\n",
    "#datasub_sub3 = dataset[['Date','Store','Dept','weeklySales','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5']]\n",
    "#dataset_sub3 = dataset_sub3.sort_index(axis=0)\n",
    "#Print('\\n\\n', dataset_sub3.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b11e606-c710-4bad-93ea-eacac174cbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "──────────────────────────────────────────────────────────────────────────────────────────────────── \n",
      "\n",
      "outlier percentage: 0.2792866941015089 ──────────────────────────────────────────────────────────────────────────────────────────────────── \n",
      "\n",
      "Store: 1 Department: 6  Removed as outlier! \n",
      "\n",
      "Store: 1 Department: 18  Removed as outlier! \n",
      "\n",
      "Store: 1 Department: 39  Removed as outlier! \n",
      "\n",
      "Store: 1 Department: 43  Removed as outlier! \n",
      "\n",
      "Store: 1 Department: 45  Removed as outlier! \n",
      "\n",
      "Store: 1 Department: 47  Removed as outlier! \n",
      "\n",
      "Store: 1 Department: 48  Removed as outlier! \n",
      "\n",
      "Store: 1 Department: 50  Removed as outlier! \n",
      "\n",
      "Store: 1 Department: 51  Removed as outlier! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3-Data cleaning\n",
    "\n",
    "# 3-1 this function identifies departmets in different stores that have incomplete data or have below 0 sales values. the function then makes a dictionary of outliers(Test:OK)\n",
    "def outlier_identifier(df, border_value, store_list, dept_list):\n",
    "    data_map = pd.DataFrame(columns=['store', 'dept', 'number_of_entries', 'target_false_count', 'outlier_flag'])\n",
    "    for i in store_list:\n",
    "        for j in dept_list:\n",
    "            number_of_entries = df[(df.Store == i) & (df.Dept == j)].Date.count()\n",
    "            number_of_entries = number_of_entries.astype('int16')\n",
    "            target_false_count = df[(df.weeklySales <= border_value) & (df.Store == \n",
    "                                                                        i) & (df.Dept == j)].weeklySales.count()\n",
    "            target_false_count = target_false_count.astype('int16')\n",
    "            if (number_of_entries == 143) & (target_false_count == 0):\n",
    "                outlier_flag = 0\n",
    "            else:\n",
    "                outlier_flag = 1\n",
    "            new_row = {'store': i, 'dept': j, 'number_of_entries': number_of_entries, 'target_false_count': target_false_count, 'outlier_flag': outlier_flag}\n",
    "            data_map.loc[len(data_map)] = new_row\n",
    "    return data_map\n",
    "\n",
    "# 3-2 this function removes departmets in different stores that have incomplete data or have below 0 sales values.(Test:OK)\n",
    "def outlier_remover(df, removal_map):\n",
    "    for i in removal_map.index:\n",
    "        a = removal_map.iloc[[i]].store\n",
    "        a.reset_index(drop=True, inplace=True)\n",
    "        b = removal_map.iloc[[i]].dept\n",
    "        b.reset_index(drop=True, inplace=True)\n",
    "        print('Store:', a[0], 'Department:', b[0],' Removed as outlier!','\\n')\n",
    "        index = df[(df.Store == a[0]) & (df.Dept == b[0])].index\n",
    "        for j in index:\n",
    "            df.drop(j , inplace=True)\n",
    "    return df\n",
    "\n",
    "# 3-3 Executing outlier identifier and save it as a mapping dataframe to know which store and department mix should be droped(Test:OK)\n",
    "data_map = outlier_identifier(dataset_sub1, 0, store_list, dept_list)\n",
    "removal_map = data_map[['store','dept','outlier_flag']]\n",
    "removal_map = removal_map[removal_map.outlier_flag == 1]\n",
    "removal_map.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# 3-4 Printing percentage of outlier data in compare with whole data\n",
    "print('outlier percentage:', data_map[data_map.outlier_flag == 1].store.count() / 3645 ,'─' * 100, '\\n')\n",
    "\n",
    "# 3-5 Executing outlier remover\n",
    "dataset_sub2 = outlier_remover(dataset_sub1, removal_map)\n",
    "\n",
    "# 3-6 Filling empty numeric values with 0  &  reseting index\n",
    "dataset_sub2 = dataset_sub2.fillna(0)\n",
    "dataset_sub2 = dataset_sub2.reset_index(drop=True)\n",
    "\n",
    "# 3-7 Outlier removing process is considerably time consuming, therefore we save it and recall cleaned data later.\n",
    "dataset_sub2.to_csv('temp_test/dataset_sub2_exp51.csv') \n",
    "\n",
    "# 3-8 Printing the result of data cleaning process\n",
    "print('─' * 100, '\\n Cleaned Dataset: \\n', dataset_sub2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2495b9c0-952b-48e1-8744-a20e61ebb2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Departments: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 16 17 19 20 21 22 23 24 25 26\n",
      " 27 28 29 30 31 32 33 34 35 36 37 38 40 41 42 44 46 48 49 50 52 55 56 58\n",
      " 59 60 65 67 71 72 74 79 80 81 82 83 85 87 90 91 92 93 94 95 96 97 98] \n",
      "\n",
      "──────────────────────────────────────────────────────────────────────────────────────────────────── \n",
      " List of Stores: \n",
      " [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45] \n",
      "\n",
      "──────────────────────────────────────────────────────────────────────────────────────────────────── \n",
      " List of Dates: \n",
      " ['2010-02-05' '2010-02-12' '2010-02-19' '2010-02-26' '2010-03-05'\n",
      " '2010-03-12' '2010-03-19' '2010-03-26' '2010-04-02' '2010-04-09'\n",
      " '2010-04-16' '2010-04-23' '2010-04-30' '2010-05-07' '2010-05-14'\n",
      " '2010-05-21' '2010-05-28' '2010-06-04' '2010-06-11' '2010-06-18'\n",
      " '2010-06-25' '2010-07-02' '2010-07-09' '2010-07-16' '2010-07-23'\n",
      " '2010-07-30' '2010-08-06' '2010-08-13' '2010-08-20' '2010-08-27'\n",
      " '2010-09-03' '2010-09-10' '2010-09-17' '2010-09-24' '2010-10-01'\n",
      " '2010-10-08' '2010-10-15' '2010-10-22' '2010-10-29' '2010-11-05'\n",
      " '2010-11-12' '2010-11-19' '2010-11-26' '2010-12-03' '2010-12-10'\n",
      " '2010-12-17' '2010-12-24' '2010-12-31' '2011-01-07' '2011-01-14'\n",
      " '2011-01-21' '2011-01-28' '2011-02-04' '2011-02-11' '2011-02-18'\n",
      " '2011-02-25' '2011-03-04' '2011-03-11' '2011-03-18' '2011-03-25'\n",
      " '2011-04-01' '2011-04-08' '2011-04-15' '2011-04-22' '2011-04-29'\n",
      " '2011-05-06' '2011-05-13' '2011-05-20' '2011-05-27' '2011-06-03'\n",
      " '2011-06-10' '2011-06-17' '2011-06-24' '2011-07-01' '2011-07-08'\n",
      " '2011-07-15' '2011-07-22' '2011-07-29' '2011-08-05' '2011-08-12'\n",
      " '2011-08-19' '2011-08-26' '2011-09-02' '2011-09-09' '2011-09-16'\n",
      " '2011-09-23' '2011-09-30' '2011-10-07' '2011-10-14' '2011-10-21'\n",
      " '2011-10-28' '2011-11-04' '2011-11-11' '2011-11-18' '2011-11-25'\n",
      " '2011-12-02' '2011-12-09' '2011-12-16' '2011-12-23' '2011-12-30'\n",
      " '2012-01-06' '2012-01-13' '2012-01-20' '2012-01-27' '2012-02-03'\n",
      " '2012-02-10' '2012-02-17' '2012-02-24' '2012-03-02' '2012-03-09'\n",
      " '2012-03-16' '2012-03-23' '2012-03-30' '2012-04-06' '2012-04-13'\n",
      " '2012-04-20' '2012-04-27' '2012-05-04' '2012-05-11' '2012-05-18'\n",
      " '2012-05-25' '2012-06-01' '2012-06-08' '2012-06-15' '2012-06-22'\n",
      " '2012-06-29' '2012-07-06' '2012-07-13' '2012-07-20' '2012-07-27'\n",
      " '2012-08-03' '2012-08-10' '2012-08-17' '2012-08-24' '2012-08-31'\n",
      " '2012-09-07' '2012-09-14' '2012-09-21' '2012-09-28' '2012-10-05'\n",
      " '2012-10-12' '2012-10-19' '2012-10-26']\n"
     ]
    }
   ],
   "source": [
    "# 4-Reading cleaned dataset and updating some lists after cleaning\n",
    "\n",
    "# 4-1 Reading saved clean data from memory\n",
    "dataset_sub2 = pd.read_csv(\"temp_test/dataset_sub2_exp51.csv\", names=['Date','Dept','Store', 'Type','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5','isHoliday','weeklySales'],sep=',', header=0)\n",
    "dataset_sub3 = dataset_sub2[['Date','Dept','weeklySales']]\n",
    "\n",
    "# 4-2 Updating list of unique departments' values\n",
    "dept_list = dataset_sub2['Dept'].unique()\n",
    "dept_list.sort()\n",
    "\n",
    "# 4-3 Updating list of unique stores' values\n",
    "store_list = dataset_sub2['Store'].unique()\n",
    "store_list.sort()\n",
    "\n",
    "# 4-4 Updating list of unique dates\n",
    "date_list = dataset_sub2['Date'].unique()\n",
    "date_list.sort()\n",
    "\n",
    "print('List of Departments:',dept_list,'\\n')\n",
    "print('─' * 100, '\\n List of Stores: \\n',store_list,'\\n')\n",
    "print('─' * 100, '\\n List of Dates: \\n',date_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b5dcc44-2ffe-4234-8a78-ee348b2ada2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-Defining experiment process, models, and methods\n",
    "\n",
    "# 5-1 This function aggregates stores weekly sales(Test:OK)\n",
    "def aggregator(df):\n",
    "    aggr = df.groupby(['Date','Dept'], as_index=False).sum()\n",
    "    return aggr\n",
    "\n",
    "# 5-2 This function gets a ataframe input and gives a dataframe output with transformed features. (Test:OK) \n",
    "# also the function reduces data types to minimum ram needed\n",
    "def create_features(df):\n",
    "    features = df\n",
    "    features['Date'] = pd.to_datetime(df['Date'])\n",
    "    features['dayofweek'] = df['Date'].dt.dayofweek\n",
    "    features['quarter'] = df['Date'].dt.quarter\n",
    "    features['month'] = df['Date'].dt.month\n",
    "    features['year'] = df['Date'].dt.year\n",
    "    features['dayofyear'] = df['Date'].dt.dayofyear\n",
    "    features['dayofmonth'] = df['Date'].dt.day\n",
    "    features['weekofyear'] = df['Date'].dt.weekofyear\n",
    "\n",
    "    cols_int16 = ['dayofweek','quarter','month','year','dayofyear','dayofmonth','weekofyear', 'Dept']\n",
    "    for col in cols_int16:\n",
    "        features['{}'.format(col)] = df['{}'.format(col)].astype('int16')\n",
    "\n",
    "    cols_float32 = ['weeklySales']\n",
    "    for col in cols_float32:\n",
    "        features['{}'.format(col)] = df['{}'.format(col)].astype('float32')\n",
    "        \n",
    "    X = features[['Date','dayofweek','quarter','month','year','dayofyear','dayofmonth','weekofyear', 'Dept', 'weeklySales']]\n",
    "    X.index = features.index\n",
    "    print('\\n >>features: \\n',X)\n",
    "    return X\n",
    "\n",
    "# 5-3 This function derives train and test datasets from a tmie-series database due to an input date(Test:OK)\n",
    "def split_data(df, split_date):\n",
    "    return df[df.Date < split_date].copy(), \\\n",
    "            df[df.Date >= split_date].copy()\n",
    "    \n",
    "# 5-4 This function plots test and train values of target in time (Test:NA)\n",
    "def plt_test_train(df_train, df_test):\n",
    "    plt.figure(figsize = (20,10))\n",
    "    plt.xlabel('date')\n",
    "    plt.ylabel('weekly sales')\n",
    "    plt.plot(df_train.index, df_train['weeklySales'],label = 'train')\n",
    "    plt.plot(df_test.index, df_test['weeklySales'], label ='test')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    msg = 'PLT Done ! \\n'\n",
    "    return msg\n",
    "\n",
    "# 5-5 This function creates, tunes, plots, finalizes, predicts, and evaluates all models in mdls list for a set of data (Test:NA)\n",
    "def create_models(mdls, test, result_log):\n",
    "    for mdl in mdls:\n",
    "        mdll = create_model('{}'.format(mdl))\n",
    "        print('\\n \\n >>mdll = create_model(mdl) for Model:{} IS  DONE! \\n \\n'.format(mdl))\n",
    "        tuned_mdl = tune_model(mdll, n_iter = 2)\n",
    "        print('\\n \\n >>tuned_mdl = tune_model(mdll) for Model:{} IS  DONE! \\n \\n'.format(mdl))\n",
    "        \n",
    "        #final_model_name = f'final_{mdl}'\n",
    "        final_model = finalize_model(tuned_mdl)\n",
    "        print('\\n \\n >>final_mdl = finalize_model(tuned_mdl) for Model:{} IS  DONE! \\n \\n'.format(mdl))\n",
    "        \n",
    "        plt(tuned_mdl)\n",
    "        \n",
    "        print(final_model)\n",
    "        print('\\n \\n >>print(final_mdl) for Model:{} IS  DONE! \\n \\n'.format(mdl))\n",
    "        \n",
    "        #save_model(final_model, f'{mdl}')\n",
    "        pred = predict_model(final_model, data=test)\n",
    "        pred.to_csv('pred_output/exp52/{}_pred.csv'.format(mdl))\n",
    "        print('\\n \\n >>pred_mdl = predict_model(final_mdl, data=test) for Model:{} IS  DONE! \\n \\n'.format(mdl))\n",
    "\n",
    "        result_log = result_logger(mdl, pred, result_log)\n",
    "        print('\\n \\n >> Prediction with Model:{}  IS  DONE!'.format(mdl))\n",
    "        \n",
    "    return result_log   \n",
    "    #pass\n",
    "\n",
    "# 5-10 This function\n",
    "def mix_methods(test, result_log):\n",
    "    \n",
    "    dt = create_model('dt')\n",
    "    tuned_dt = tune_model(dt, n_iter = 2)\n",
    "    \n",
    "    rf = create_model('rf')\n",
    "    tuned_rf = tune_model(rf, n_iter = 2)\n",
    "\n",
    "    #Blend\n",
    "    blend = blend_models([tuned_dt, tuned_rf])\n",
    "    tuned_blend = tune_model(blend, n_iter = 2)\n",
    "    \n",
    "    pred = predict_model(tuned_blend, data=test)\n",
    "    result_log = result_logger('blend', pred, result_log)\n",
    "\n",
    "    plt_mix(tuned_blend)\n",
    "    pred.to_csv('pred_output/exp52/blend_pred.csv')\n",
    "    print('\\n \\n >> Prediction with Model:blend  IS  DONE!')\n",
    "\n",
    "    #stacker\n",
    "    stacker = stack_models([tuned_dt, tuned_rf])\n",
    "    tuned_stacker = tune_model(stacker, n_iter = 2)\n",
    "    \n",
    "    pred = predict_model(tuned_stacker, data = test)\n",
    "    result_log = result_logger('stacker', pred, result_log)\n",
    "\n",
    "    plt_mix(tuned_blend)\n",
    "    pred.to_csv('pred_output/exp52/stacker_pred.csv')\n",
    "    print('\\n \\n >> Prediction with Model:stacker  IS  DONE!')   \n",
    "    \n",
    "    #bagging\n",
    "    bagged_rf = ensemble_model(tuned_rf, n_estimators=2)\n",
    "    #tuned_bagged_rf = tune_model(bagged_rf, n_iter = 2)\n",
    "    \n",
    "    pred = predict_model(bagged_rf, data = test)\n",
    "    result_log = result_logger('bagged_rf', pred, result_log)\n",
    "\n",
    "    plt_mix(bagged_rf)\n",
    "    pred.to_csv('pred_output/exp52/bagged_rf.csv')\n",
    "    print('\\n \\n >> Prediction with Model:bagged_rf  IS  DONE!')   \n",
    "    \n",
    "    #boosting\n",
    "    boosted_rf = ensemble_model(tuned_rf, method = 'Boosting')\n",
    "    #tuned_boosted_rf = tune_model(boosted_rf, n_iter = 2)\n",
    "    \n",
    "    pred = predict_model(boosted_rf, data = test)\n",
    "    result_log = result_logger('boosted_rf', pred, result_log)\n",
    "\n",
    "    plt_mix(boosted_rf)\n",
    "    pred.to_csv('pred_output/exp52/boosted_rf.csv')\n",
    "    print('\\n \\n >> Prediction with Model:boosted_rf  IS  DONE!')   \n",
    "    return result_log\n",
    "\n",
    "# 5-6 This function sets up machine-learning process configurations(Test:NA)\n",
    "def mlsetup(train, test):\n",
    "    reg = setup(data = train,\n",
    "            test_data = test,\n",
    "            target = 'weeklySales',\n",
    "            categorical_features = ['Dept','dayofweek','quarter','month','year','dayofyear','dayofmonth','weekofyear'],\n",
    "            #numeric_features = ['Date', 'Dept','Store','dayofweek','quarter','month','year','dayofyear','dayofmonth','weekofyear','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5'],\n",
    "            #preprocess = False,\n",
    "            imputation_type = None, #We dont want to impute missing values because they are alreay imputed.\n",
    "            #numeric_imputation = 'mean',\n",
    "            #polynomial_features = False, #it means we do not want to take existing features and raise them to a power to capture non-linear relationships between the feature and the target variable.\n",
    "            #transformation = False,\n",
    "            #normalize = False,\n",
    "            #normalize_method = 'zscore',\n",
    "            #transform_target = False,\n",
    "            #remove_multicollinearity = False,\n",
    "            #multicollinearity_threshold = 0.95,\n",
    "            remove_outliers = False,\n",
    "            #outliers_method = 'ee' #options are 'ee', 'lof', 'iforest',\n",
    "            #outliers_threshhold = 0.05,\n",
    "            #feature_selection = False,\n",
    "            #feature_selection_method = 'sequential',\n",
    "            #feature_selection_estimator = 'lightgbm',\n",
    "            #n_features_to_select = 0.2,\n",
    "            #use_gpu = True,\n",
    "            #profile = True,\n",
    "            #fold_strategy = 'kfold', #other options are 'kfold', 'groupkfold', 'timeseries'\n",
    "            fold = 2,  \n",
    "            #fold_groups = 'dept',\n",
    "            #data_split_shuffle = False,\n",
    "            #fold_shuffle = True,\n",
    "           )\n",
    "    print('\\n \\n >>ML setup  IS  DONE! \\n \\n')\n",
    "    #best = compare_models(sort = 'MAPE', n_select = 1)\n",
    "    #best2 = compare_models(sort = 'MAPE', n_select = 2)\n",
    "    #best3 = compare_models(sort = 'MAPE', n_select = 3)\n",
    "    #best4 = compare_models(sort = 'MAPE', n_select = 4)\n",
    "    #best5 = compare_models(sort = 'MAPE', n_select = 5)\n",
    "    #print('\\n \\n >>best = compare_models IS  DONE! \\n \\n')\n",
    "    #evaluate_model(best)\n",
    "    #print('\\n \\n >>evaluate_model(best) IS  DONE! \\n \\n')\n",
    "    #return best3\n",
    "    pass\n",
    "\n",
    "\n",
    "# 5-7 This function executes each step of the whole experiment process one by one.(Test:NA)\n",
    "#Experiment51:\n",
    "def experiment52(df, split_date, mdls, mix_mdls):\n",
    "    y = aggregator(df)\n",
    "    z = create_features(y)\n",
    "    train, test = split_data(z, split_date)\n",
    "    train2 = train.drop(columns=['Date'])\n",
    "    test2 = test.drop(columns=['Date'])\n",
    "    print('>create features and split_data func is Done! \\n')\n",
    "    \n",
    "    mlsetup(train2, test2)\n",
    "    print('\\n >mlsetup func is Done! \\n')\n",
    "\n",
    "    result_log = result_log_init(test)\n",
    "    result_log = create_models(mdls, test2, result_log)\n",
    "    result_log = mix_methods(test2, result_log)\n",
    "    result_log.to_csv('output_analysis/exp52/result_log.csv')\n",
    "\n",
    "    output_anal = output_analyzer(result_log, mdls, mix_mdls)\n",
    "    output_anal.to_csv('output_analysis/exp52/output_anal.csv')\n",
    "    \n",
    "    print('\\n >create_models func is Done! \\n')\n",
    "        \n",
    "    #result_log_aggr_exp1.to_csv('output_analysis/exp1/result_log_aggr_exp1.csv')\n",
    "    process_end_msg = '>>>>>>>>>>> Experience 52 is DONE! <<<<<<<<<<<<'\n",
    "    print(process_end_msg)\n",
    "    pass\n",
    "\n",
    "\n",
    "# 5-8 This function\n",
    "def result_logger(mdl, pred, result_log):  \n",
    "    result_log[mdl] = pred.prediction_label\n",
    "    result_log['{}_ape'.format(mdl)] = ((result_log.actual - result_log[mdl]) / result_log.actual).abs() \n",
    "    result_log['{}_pe'.format(mdl)] = ((result_log.actual - result_log[mdl]) / result_log.actual) \n",
    "    result_log['{}_pos_pe'.format(mdl)] = result_log[(result_log['{}_pe'.format(mdl)] >= 0)]['{}_pe'.format(mdl)]\n",
    "    result_log['{}_neg_pe'.format(mdl)] = result_log[(result_log['{}_pe'.format(mdl)] < 0)]['{}_pe'.format(mdl)]\n",
    "    result_log['{}_err_p2'.format(mdl)] = ((result_log.actual - result_log[mdl]) ** 2)\n",
    "    return result_log\n",
    "\n",
    "\n",
    "# 5-8 This function\n",
    "def result_log_init(test_df):  \n",
    "    #initiating result log \n",
    "    result_log = pd.DataFrame()\n",
    "    result_log.index = test_df.index\n",
    "    result_log['Date'] = test_df['Date']\n",
    "    result_log['Dept'] = test_df['Dept']\n",
    "    result_log['actual'] = test_df['weeklySales'] \n",
    "    return result_log\n",
    "\n",
    "\n",
    "# 5-9 This function\n",
    "def output_analyzer(result_log, mdls, mix_mdls):\n",
    "    output_anal = pd.DataFrame()\n",
    "    mdls_and_mix = mdls + mix_mdls\n",
    "    output_anal.index = mdls_and_mix\n",
    "\n",
    "    for mdl in mdls_and_mix:\n",
    "        pos_pe_mean = result_log['{}_pos_pe'.format(mdl)].mean()\n",
    "        max_pos_pe = result_log['{}_pos_pe'.format(mdl)].max()\n",
    "        neg_pe_mean = result_log['{}_neg_pe'.format(mdl)].mean()\n",
    "        max_neg_pe = result_log['{}_neg_pe'.format(mdl)].min()\n",
    "        mape = result_log['{}_ape'.format(mdl)].mean()\n",
    "        err_sum = result_log['{}_err_p2'.format(mdl)].sum()\n",
    "        err_count = result_log['{}_err_p2'.format(mdl)].count()\n",
    "        var = (err_sum / (err_count - 1))\n",
    "        sd = ((err_sum / (err_count - 1))**0.5)\n",
    "\n",
    "        output_anal.at[mdl, 'pos_pe_mean'] = pos_pe_mean\n",
    "        output_anal.at[mdl, 'max_pos_pe'] = max_pos_pe\n",
    "        output_anal.at[mdl, 'neg_pe_mean'] = neg_pe_mean\n",
    "        output_anal.at[mdl, 'max_neg_pe'] = max_neg_pe\n",
    "        output_anal.at[mdl, 'mape'] = mape\n",
    "        output_anal.at[mdl, 'var'] = var\n",
    "        output_anal.at[mdl, 'sd'] = sd\n",
    "    return output_anal\n",
    "\n",
    "\n",
    "    #output_anal_dept_best = pd.DataFrame()\n",
    "    #output_anal.index = dept_list\n",
    "\n",
    "# 5-10 This function\n",
    "def plt(mdl):\n",
    "    plot_model(mdl)\n",
    "    plot_model(mdl, plot = 'error')\n",
    "    plot_model(mdl, plot='feature')\n",
    "    plot_model(mdl, plot = 'learning')\n",
    "    plot_model(mdl, plot = 'manifold')\n",
    "    plot_model(mdl, plot = 'feature_all')\n",
    "    plot_model(mdl, plot = 'parameter')\n",
    "    #plot_model(final_blend, plot = 'error')\n",
    "\n",
    "    #interpret_model(mdl)\n",
    "    #interpret_model(mdl, plot = 'correlation')\n",
    "    #interpret_model(mdl, plot = 'reason', observation = 12)\n",
    "        \n",
    "    #plot_model(mdll)\n",
    "    #print('\\n \\n >>plot_model(mdll) for Model:{} IS  DONE! \\n \\n'.format(mdl))\n",
    "    #plot_model(mdll, plot = 'error')\n",
    "    #print('\\n \\n >>plot_model(mdll, plot = error) for Model:{} IS  DONE! \\n \\n'.format(mdl))\n",
    "    #plot_model(tuned_mdl, plot = 'feature')\n",
    "    print('\\n \\n >>plotting is  DONE! \\n \\n')\n",
    "    pass\n",
    "\n",
    "# 5-10 This function\n",
    "def plt_mix(mdl):\n",
    "    plot_model(mdl)\n",
    "    plot_model(mdl, plot = 'error')\n",
    "    #plot_model(mdl, plot='feature')\n",
    "    plot_model(mdl, plot = 'learning')\n",
    "    plot_model(mdl, plot = 'manifold')\n",
    "    #plot_model(mdl, plot = 'feature_all')\n",
    "    plot_model(mdl, plot = 'parameter')\n",
    "    #plot_model(final_blend, plot = 'error')\n",
    "\n",
    "    #interpret_model(mdl)\n",
    "    #interpret_model(mdl, plot = 'correlation')\n",
    "    #interpret_model(mdl, plot = 'reason', observation = 12)\n",
    "        \n",
    "    #plot_model(mdll)\n",
    "    #print('\\n \\n >>plot_model(mdll) for Model:{} IS  DONE! \\n \\n'.format(mdl))\n",
    "    #plot_model(mdll, plot = 'error')\n",
    "    #print('\\n \\n >>plot_model(mdll, plot = error) for Model:{} IS  DONE! \\n \\n'.format(mdl))\n",
    "    #plot_model(tuned_mdl, plot = 'feature')\n",
    "    print('\\n \\n >>plotting is  DONE! \\n \\n')\n",
    "    pass\n",
    "\n",
    "#    blend = blend_models(estimator_list = [tuned_catboost, tuned_br, tuned_rf])\n",
    "#    pred = predict_model(blend)\n",
    "#    final_blend = finalize_model(blend)\n",
    "\n",
    "#bagged_dt = ensemble_model(dt, n_estimators=50)\n",
    "#boosted_dt = ensemble_model(dt, method = 'Boosting')\n",
    "#blender = blend_models()\n",
    "#stacker = stack_models(estimator_list = compare_models(n_select=5, fold = 5, whitelist = models(type='ensemble').index.tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02c2923b-5aa6-4885-9856-5879b5e60bde",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to function call here. Maybe you meant '==' instead of '='? (1853309475.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[22], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    '{}'.format(mix_mdl) = 1\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m cannot assign to function call here. Maybe you meant '==' instead of '='?\n"
     ]
    }
   ],
   "source": [
    "for mix_mdl in mix_mdls:\n",
    "    '{}'.format(mix_mdl) = 1\n",
    "\n",
    "    print(blend)\n",
    "\n",
    "#for mix_mdl in mix_mdls:\n",
    "#    exec('final_{}'.format(mix_mdl))\n",
    "    \n",
    "#class get_models:\n",
    "#    def __init__(self, name, tunned_model)\n",
    "#        self.name = name\n",
    "#        self.final = finalize_model(tunned_model)\n",
    "#\n",
    "#    def myfunc(self):\n",
    "#        self.final = finalize_model(tunned_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330470fa-6a8e-41f3-b7b4-a789e3b50ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " >>features: \n",
      "             Date  dayofweek  quarter  month  year  dayofyear  dayofmonth  \\\n",
      "0     2010-02-05          4        1      2  2010         36           5   \n",
      "1     2010-02-05          4        1      2  2010         36           5   \n",
      "2     2010-02-05          4        1      2  2010         36           5   \n",
      "3     2010-02-05          4        1      2  2010         36           5   \n",
      "4     2010-02-05          4        1      2  2010         36           5   \n",
      "...          ...        ...      ...    ...   ...        ...         ...   \n",
      "10148 2012-10-26          4        4     10  2012        300          26   \n",
      "10149 2012-10-26          4        4     10  2012        300          26   \n",
      "10150 2012-10-26          4        4     10  2012        300          26   \n",
      "10151 2012-10-26          4        4     10  2012        300          26   \n",
      "10152 2012-10-26          4        4     10  2012        300          26   \n",
      "\n",
      "       weekofyear  Dept   weeklySales  \n",
      "0               5     1  8.818334e+05  \n",
      "1               5     2  1.997832e+06  \n",
      "2               5     3  4.843689e+05  \n",
      "3               5     4  1.205802e+06  \n",
      "4               5     5  1.116258e+06  \n",
      "...           ...   ...           ...  \n",
      "10148          43    94  1.264738e+06  \n",
      "10149          43    95  3.002617e+06  \n",
      "10150          43    96  5.164860e+05  \n",
      "10151          43    97  5.875408e+05  \n",
      "10152          43    98  2.957347e+05  \n",
      "\n",
      "[10153 rows x 10 columns]\n",
      ">create features and split_data func is Done! \n",
      "\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f9dd9_row9_col1 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f9dd9\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f9dd9_level0_col0\" class=\"col_heading level0 col0\" >Description</th>\n",
       "      <th id=\"T_f9dd9_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f9dd9_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_f9dd9_row0_col0\" class=\"data row0 col0\" >Session id</td>\n",
       "      <td id=\"T_f9dd9_row0_col1\" class=\"data row0 col1\" >7700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9dd9_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_f9dd9_row1_col0\" class=\"data row1 col0\" >Target</td>\n",
       "      <td id=\"T_f9dd9_row1_col1\" class=\"data row1 col1\" >weeklySales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9dd9_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_f9dd9_row2_col0\" class=\"data row2 col0\" >Target type</td>\n",
       "      <td id=\"T_f9dd9_row2_col1\" class=\"data row2 col1\" >Regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9dd9_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_f9dd9_row3_col0\" class=\"data row3 col0\" >Original data shape</td>\n",
       "      <td id=\"T_f9dd9_row3_col1\" class=\"data row3 col1\" >(10153, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9dd9_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_f9dd9_row4_col0\" class=\"data row4 col0\" >Transformed data shape</td>\n",
       "      <td id=\"T_f9dd9_row4_col1\" class=\"data row4 col1\" >(10153, 23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9dd9_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_f9dd9_row5_col0\" class=\"data row5 col0\" >Transformed train set shape</td>\n",
       "      <td id=\"T_f9dd9_row5_col1\" class=\"data row5 col1\" >(6319, 23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9dd9_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_f9dd9_row6_col0\" class=\"data row6 col0\" >Transformed test set shape</td>\n",
       "      <td id=\"T_f9dd9_row6_col1\" class=\"data row6 col1\" >(3834, 23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9dd9_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_f9dd9_row7_col0\" class=\"data row7 col0\" >Ordinal features</td>\n",
       "      <td id=\"T_f9dd9_row7_col1\" class=\"data row7 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9dd9_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_f9dd9_row8_col0\" class=\"data row8 col0\" >Categorical features</td>\n",
       "      <td id=\"T_f9dd9_row8_col1\" class=\"data row8 col1\" >8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9dd9_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_f9dd9_row9_col0\" class=\"data row9 col0\" >Preprocess</td>\n",
       "      <td id=\"T_f9dd9_row9_col1\" class=\"data row9 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9dd9_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_f9dd9_row10_col0\" class=\"data row10 col0\" >Imputation type</td>\n",
       "      <td id=\"T_f9dd9_row10_col1\" class=\"data row10 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9dd9_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_f9dd9_row11_col0\" class=\"data row11 col0\" >Maximum one-hot encoding</td>\n",
       "      <td id=\"T_f9dd9_row11_col1\" class=\"data row11 col1\" >25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9dd9_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_f9dd9_row12_col0\" class=\"data row12 col0\" >Encoding method</td>\n",
       "      <td id=\"T_f9dd9_row12_col1\" class=\"data row12 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9dd9_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_f9dd9_row13_col0\" class=\"data row13 col0\" >Fold Generator</td>\n",
       "      <td id=\"T_f9dd9_row13_col1\" class=\"data row13 col1\" >KFold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9dd9_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_f9dd9_row14_col0\" class=\"data row14 col0\" >Fold Number</td>\n",
       "      <td id=\"T_f9dd9_row14_col1\" class=\"data row14 col1\" >2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9dd9_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_f9dd9_row15_col0\" class=\"data row15 col0\" >CPU Jobs</td>\n",
       "      <td id=\"T_f9dd9_row15_col1\" class=\"data row15 col1\" >-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9dd9_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_f9dd9_row16_col0\" class=\"data row16 col0\" >Use GPU</td>\n",
       "      <td id=\"T_f9dd9_row16_col1\" class=\"data row16 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9dd9_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_f9dd9_row17_col0\" class=\"data row17 col0\" >Log Experiment</td>\n",
       "      <td id=\"T_f9dd9_row17_col1\" class=\"data row17 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9dd9_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_f9dd9_row18_col0\" class=\"data row18 col0\" >Experiment Name</td>\n",
       "      <td id=\"T_f9dd9_row18_col1\" class=\"data row18 col1\" >reg-default-name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9dd9_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_f9dd9_row19_col0\" class=\"data row19 col0\" >USI</td>\n",
       "      <td id=\"T_f9dd9_row19_col1\" class=\"data row19 col1\" >adb2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2766a2c56d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " >>ML setup  IS  DONE! \n",
      " \n",
      "\n",
      "\n",
      " >mlsetup func is Done! \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Initiated</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>23:28:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Status</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>Fitting 2 Folds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Estimator</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>Decision Tree Regressor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       \n",
       "                                                                       \n",
       "Initiated  . . . . . . . . . . . . . . . . . .                 23:28:00\n",
       "Status     . . . . . . . . . . . . . . . . . .          Fitting 2 Folds\n",
       "Estimator  . . . . . . . . . . . . . . . . . .  Decision Tree Regressor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ea10b5ded042b49ceab012491a2323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6-Setting parameters and executing the experiment\n",
    "#mdls = ['xgboost', 'catboost', 'tr', 'lightgbm', 'gbr', 'huber', 'ada', 'par', 'omp', 'en', 'lasso', 'llar', 'br', 'ridge', 'lar', 'lr','dt', 'rf', 'et']\n",
    "# excluded: 'dummy', 'knn', 'ransac', 'tr', 'kr', 'svm'\n",
    "mdls = ['dt', 'rf']\n",
    "\n",
    "#['blend', 'bag', 'boost', 'stack']\n",
    "mix_mdls = ['blend', 'stacker', 'bagged_rf', 'boosted_rf']\n",
    "\n",
    "experiment52(dataset_sub3, '2011-10-19', mdls, mix_mdls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
